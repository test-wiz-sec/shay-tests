"source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "TOKEN = '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'\n",
    "\n",
    "CAUSALLM_MODELS = [\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"decapoda-research/llama-7b-hf\",\n",
    "    \"ybelkada/falcon-7b-sharded-bf16\",\n",
    "    \"facebook/galactica-1.3b\",\n",
    "    \"gpt2\",\n",
    "    \"microsoft/biogpt\",\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"bn22/Mistral-7B-Instruct-v0.1-sharded\",\n",
    "    \"vilsonrodrigues/falcon-7b-sharded\",\n",
    "    \"TinyPixel/Llama-2-7B-bf16-sharded\",\n",
    "    \"stanford-crfm/BioMedLM\",\n",
    "]  # Define model names that are CausalLM models\n",
    "\n",
